{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZ1tRYX/l7/X7GZJBXL/qd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CS23M005/DL_A1/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xn2oKKKb95fW"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "\n",
        "from FFNN import *\n",
        "\n",
        "(X_train1, y_train1), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "kk = len(X_train1)\n",
        "xt = np.split(X_train1[0:kk], [int(len(X_train1[0:kk])*0.9)])\n",
        "yt = np.split(y_train1[0:kk], [int(len(y_train1[0:kk])*0.9)])\n",
        "X_train = xt[0]/255\n",
        "X_validation = xt[1]/255\n",
        "y_train = yt[0]\n",
        "y_validation = yt[1]\n",
        "\n",
        "\n",
        "sweep_config = {\"name\": \"complete-sweep-25dw\", \"method\": \"random\"}\n",
        "sweep_config[\"metric\"] = {\"name\": \"validation_acc\", \"goal\": \"maximize\"}\n",
        "parameters_dict = {\n",
        "                \"num_epochs\": {\"values\": [2, 3, 4, 5]}, \\\n",
        "                \"num_hidden_layers\": {\"values\": [2, 3, 4]}, \\\n",
        "                \"size_hidden_layer\": {\"values\": [16, 32, 64, 128]}, \\\n",
        "                \"learning_rate\": {\"values\": [ 1e-2, 1e-3, 1e-4]}, \\\n",
        "                \"optimizer\": {\"values\": [\"sgd\", \"adam\", \"mgd\", \"nadam\", \"rmsprop\", \"nag\"]}, \\\n",
        "                \"batch_size\": {\"values\": [32, 64, 128]}, \\\n",
        "                \"weight_init\": {\"values\": [\"random\", \"xavier\"]} , \\\n",
        "                \"activation\": {\"values\": [\"sigmoid\", \"tanh\", \"relu\"]}, \\\n",
        "                \"loss\": {\"values\": [\"crossentropy\"]}, \\\n",
        "                \"reg_lambda\": {\"values\": [0.001, 0.0005, 0]}, \\\n",
        "                  }\n",
        "sweep_config[\"parameters\"] = parameters_dict\n",
        "\n",
        "\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config,project='Assignment1', entity='cs23m005')\n",
        "\n",
        "def train_nn():\n",
        "    config_defaults = {\n",
        "            \"num_epochs\": 3, \\\n",
        "            \"num_hidden_layers\":  4, \\\n",
        "            \"size_hidden_layer\": 128, \\\n",
        "            \"learning_rate\": 1e-3, \\\n",
        "            \"optimizer\": \"mgd\", \\\n",
        "            \"batch_size\": 32, \\\n",
        "            \"weight_init\": \"xavier\" , \\\n",
        "            \"activation\": \"tanh\", \\\n",
        "            \"loss\": \"crossentropy\", \\\n",
        "            \"reg_lambda\": 0.001, \\\n",
        "    }\n",
        "    wandb.init(config = config_defaults, project = \"Assignment1\")\n",
        "    config = wandb.config\n",
        "    print(config.optimizer)\n",
        "    wandb.run.name = \"e_{}_hl_{}_opt_{}_bs_{}_init_{}_ac_{}_loss_{}\".format(config.num_epochs,\\\n",
        "                                                                  config.num_hidden_layers,\\\n",
        "                                                                  config.size_hidden_layer,\\\n",
        "                                                                  config.learning_rate,\\\n",
        "                                                                  config.optimizer,\\\n",
        "                                                                  config.batch_size,\\\n",
        "                                                                  config.weight_init,\\\n",
        "                                                                  config.activation,\\\n",
        "                                                                  config.loss,\\\n",
        "                                                                  config.reg_lambda)\n",
        "\n",
        "    seed(42)\n",
        "    n_inputs = 784\n",
        "    n_layers = config.num_hidden_layers\n",
        "    n_neurons = config.size_hidden_layer\n",
        "    n_outputs = 10\n",
        "    init_mode = config.weight_init\n",
        "    w,b = init_nn(n_inputs, n_layers, n_neurons, n_outputs, init_mode)\n",
        "    w_old = w.copy()\n",
        "    b_old = b.copy()\n",
        "\n",
        "    eta = config.learning_rate\n",
        "    activation_f = config.activation\n",
        "    batch_size = config.batch_size\n",
        "    loss = config.loss\n",
        "    ud_lambda = config.reg_lambda\n",
        "    optimizer = config.optimizer\n",
        "\n",
        "    beta = 0.9\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.999\n",
        "\n",
        "    eps = 1e-9\n",
        "    max_epochs = config.num_epochs\n",
        "    momentum = 0.9\n",
        "\n",
        "    if optimizer == \"sgd\":\n",
        "      w_n,b_n, a_n, h_n = sgd(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs)\n",
        "    elif optimizer == \"mgd\":\n",
        "      w_n,b_n, a_n, h_n = mgd(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, max_epochs, momentum)\n",
        "    elif optimizer == \"nag\":\n",
        "      w_n,b_n, a_n, h_n = nag(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, momentum)\n",
        "    elif optimizer == \"rmsprop\":\n",
        "      w_n,b_n, a_n, h_n = rmsprop(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, beta, eps)\n",
        "    elif optimizer == \"adam\":\n",
        "      w_n,b_n, a_n, h_n = adam(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, beta1, beta2, eps)\n",
        "    elif optimizer == \"nadam\":\n",
        "      w_n,b_n, a_n, h_n = nadam(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, beta1, beta2, eps)\n",
        "\n",
        "\n",
        "    train_accuracy, yp = accuracy(X_train, y_train, w_n, b_n, n_layers, activation_f)\n",
        "    validation_accuracy, yp = accuracy(X_validation, y_validation, w_n, b_n, n_layers, activation_f)\n",
        "    test_accuracy, ytp = accuracy(X_test, y_test, w_n, b_n, n_layers, activation_f)\n",
        "\n",
        "\n",
        "    wandb.log({\"train_acc\": train_accuracy, \\\n",
        "               \"validation_acc\": validation_accuracy, \\\n",
        "                \"test_acc\": test_accuracy, \\\n",
        "                })\n",
        "    print(\"accu: \", validation_accuracy)\n",
        "\n",
        "\n",
        "####################################################################\n",
        "\n",
        "#sweep_id = wandb.sweep(sweep_config, project = \"Assignment1\")\n",
        "#wandb.agent(sweep_id, function = train_nn, count=25)\n",
        "# wandb.finish()\n",
        "#sweep_id = wandb.sweep(project = \"Assignment1\")\n",
        "#wandb.agent(function = train_nn)\n",
        "#wandb.finish()\n",
        "#train_nn()\n",
        "#wandb.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    __spec__ = \"ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>)\"\n",
        "    wandb.agent(sweep_id, train_nn, count = 1)\n",
        "    wandb.finish()\n",
        ""
      ]
    }
  ]
}