# -*- coding: utf-8 -*-
"""FFNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DRpVwKNfmdHEdgFRE0gYA_sbuyrsTUkO
"""

!pip install wandb
import wandb
wandb.login()

import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import fashion_mnist
from keras.datasets import mnist
import random

(X_train1, y_train1), (X_test, y_test) = fashion_mnist.load_data()
# X_train = X_train1[0:2000]/255
# X_validation = X_train1[2001:2100]/255
# y_train = y_train1[0:2000]
# y_validation = y_train1[2001:2100]

kk = len(X_train1)
xt = np.split(X_train1[0:kk], [int(len(X_train1[0:kk])*0.9)])
yt = np.split(y_train1[0:kk], [int(len(y_train1[0:kk])*0.9)])
X_train = xt[0]/255
X_validation = xt[1]/255
y_train = yt[0]
y_validation = yt[1]

# (X_train1, y_train1), (X_test, y_test) = fashion_mnist.load_data()
# X_train = X_train1[0:40000]/255
# X_validation = X_train1[40001:50000]/255
# y_train = y_train1[0:40000]
# y_validation = y_train1[40001:50000]

samples_for_print = [];
unique_labels = np.unique(y_train)
u_l = np.copy(unique_labels)

while(len(unique_labels)>0):
  for i in range (0, len(y_train)):
    if(y_train[i] == unique_labels[0]):
      samples_for_print.append(X_train[i])
      unique_labels = np.setdiff1d(unique_labels,unique_labels[0])
      break

i = 1
rows = 2
cols = int(len(samples_for_print)/rows)
fig1 = plt.figure(figsize = (4,2))
for img in samples_for_print:
    plt.subplot(rows, cols, i)
    plt.imshow(img, cmap='gray')
    plt.title(u_l[i-1])
    plt.tick_params(left = False, right = False , labelleft = False ,
                labelbottom = False, bottom = False)
    i = i+1
plt.show()

from random import seed
import random

def init_random(n1, n2, init_mode, xav_std):
  if init_mode == "random":
    return [[np.random.normal(0,1) for i in range(n1)] for j in range(n2)]
  elif init_mode == "xavier":
    return [[np.random.normal(0,xav_std) for i in range(n1)] for j in range(n2)]

def init_nn(n_inputs, n_layers, n_neurons, n_outputs, init_mode):
  #print("in weight init")
  w = [0]*(n_layers+2)
  biases = []
  xavier_stddev = np.sqrt(2 / (n_inputs + n_outputs))
  input_layer = init_random(n_inputs, n_neurons, init_mode, xavier_stddev)
  w[1] = input_layer
  biases.append([0])
  biases.append(init_random(n_neurons, 1, init_mode, xavier_stddev)[0])
  for i in range(2,n_layers+1):
    hidden_layer = init_random(n_neurons, n_neurons, init_mode, xavier_stddev)
    w[i] = hidden_layer
    biases.append(init_random(n_neurons, 1, init_mode, xavier_stddev)[0])
  output_layer = init_random(n_neurons, n_outputs, init_mode, xavier_stddev)
  w[n_layers+1] = output_layer
  biases.append(init_random(n_outputs, 1, init_mode, xavier_stddev)[0])
  return w, biases

def sigmoid(x):
  x = np.clip(x, -709.78, 709.78)
  return 1. / (1.+np.exp(-x))

def sigmoid_derivative(x):
  x = np.clip(x, -709.78, 709.78)
  return sigmoid(x) * (1-sigmoid(x))

def Relu(x):
  return np.maximum(0,x)

def Relu_derivative(x):
  return 1*(x>0)

def tanh(x):
  return np.tanh(x)

def tanh_derivative(x):
  return (1 - (np.tanh(x)**2))

def softmax(x):
  x = np.clip(x, -709.78, 709.78)
  return np.exp(x) / np.sum(np.exp(x), axis=0)

def softmax_derivative(x):
  x = np.clip(x, -709.78, 709.78)
  return softmax(x) * (1-softmax(x))

def oneHotEncode(y_actual, n_outputs):
  Ydata = np.zeros(n_outputs)
  Ydata[int(y_actual)] = 1
  return Ydata

# def usigmoid(x):
#   x = np.array(x, dtype=np.float128)
#   return 1. / (1.+np.exp(-x))
# x = np.array([1235, -1235])
# usigmoid(x)

def forward_prop(w, b, sample, n_layers, activation_f):
  a = []
  h = []
  h.append(sample)
  a.append([0])
  for i in range(1, n_layers+1):
    a.append(np.matmul(np.array(w[i]),np.array(h[i-1]))+b[i])
    if activation_f == 'sigmoid':
      h.append(sigmoid(a[i]))
    elif activation_f == 'relu':
      h.append(Relu(a[i]))
    elif activation_f == 'tanh':
      h.append(tanh(a[i]))
  a.append(np.matmul(w[n_layers+1],h[n_layers])+b[n_layers+1])
  y = softmax(a[n_layers+1])
  return y, a, h

def backprop(y_hat, y_actual, a, h, w, b, activation_f, batch_size, loss, ud_lambda, n_outputs):

    L = len(w)-1
    gradient_a = [0]*(L+1)
    gradient_w = [0]*(L+1)
    gradient_b = [0]*(L+1)
    gradient_h = [0]*(L+1)
    y = oneHotEncode(y_actual, n_outputs)

    if loss == 'crossentropy':
      gradient_a[L] = y_hat-y
    elif loss == 'mse':
      gradient_a[L] = np.multiply(2 * (y_hat - y), np.multiply(y_hat, (1 - y_hat)))

    for k in range(L,0,-1):
      gradient_w[k] = (np.outer(gradient_a[k], h[k-1].transpose()))
      gradient_b[k] = gradient_a[k]

      if (k > 1):
        if activation_f == 'sigmoid':
          gradient_h[k-1] = np.matmul(np.array(w[k]).transpose(), np.array(gradient_a[k]))
          gradient_a[k-1]  =np.array(gradient_h[k-1]) * np.array(sigmoid_derivative(a[k-1]))

        elif activation_f == 'relu':
          gradient_h[k-1] = np.matmul(np.array(w[k]).transpose(), np.array(gradient_a[k]))
          gradient_a[k-1]  =np.array(gradient_h[k-1]) * np.array(Relu_derivative(a[k-1]))

        elif activation_f == 'tanh':
          gradient_h[k-1] = np.matmul(np.array(w[k]).transpose(), np.array(gradient_a[k]))
          gradient_a[k-1]  =np.array(gradient_h[k-1]) * np.array(tanh_derivative(a[k-1]))

    return gradient_w, gradient_b

def accuracy(x_data,y_data, w, b, n_layers, activation_f):
  #print("in accuracy")
  count = 0
  y_p = []
  for x,y in zip(x_data,y_data):
    sample = x.flatten()
    y_hat, a_dum, h_dum = forward_prop(w, b, sample, n_layers, activation_f)
    y_pred = np.argmax(y_hat, axis=0)
    y_p.append(y_pred)
    if y_pred == y:
      count += 1
  accuracy = count/len(y_data)
  return accuracy, y_p

def sgd(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, use_wandb, max_epochs):
  print("in sgd")
  L = len(w)-1
  dw = [0]*(L+1)
  db = [0]*(L+1)

  for i in range(max_epochs):
    print("epoch", i)
    for ii in range(1, n_layers+2):
      dw[ii] = np.zeros_like(w[ii])
      db[ii] = np.zeros_like(b[ii])
    num_points_seen = 0
    for x,y in zip(X_train, y_train):
      sample = x.flatten()
      y_hat, a, h = forward_prop(w, b, sample, n_layers, activation_f)
      temp_dw, temp_db = backprop(y_hat, y, a, h, w, b, activation_f, batch_size, loss, ud_lambda, n_outputs)
      #print("dw, db init happened", np.array(dw[1]).shape)
      num_points_seen = num_points_seen + 1
      for j in range(1, n_layers+2):
          dw[j] = dw[j]+ np.array(temp_dw[j])
          db[j] = db[j]+ np.array(temp_db[j])
      #print(num_points_seen)
      if (num_points_seen % batch_size) == 0:
        #print("update rule start: ", num_points_seen)
        for j in range(1, n_layers+2):
          w[j] = (w[j]-eta*dw[j])
          b[j] = b[j]-eta*db[j]
        #print("update rule end: ", num_points_seen)
        for ii in range(1, n_layers+2):
          dw[ii] = np.zeros_like(w[ii])
          db[ii] = np.zeros_like(b[ii])

  return w,b,a,h

def nadam(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, use_wandb, max_epochs, beta1, beta2, eps):
  print("in nadam")
  L = len(w)-1
  dw = [0]*(L+1)
  db = [0]*(L+1)
  mw = [0]*(L+1)
  mb = [0]*(L+1)
  vw = [0]*(L+1)
  vb = [0]*(L+1)
  for ii in range(1, n_layers+2):
    vw[ii] = np.zeros_like(w[ii])
    vb[ii] = np.zeros_like(b[ii])
    mw[ii] = np.zeros_like(w[ii])
    mb[ii] = np.zeros_like(b[ii])

  for i in range(max_epochs):
    print("epoch", i)
    for ii in range(1, n_layers+2):
      dw[ii] = np.zeros_like(w[ii])
      db[ii] = np.zeros_like(b[ii])
    num_points_seen = 0
    for x,y in zip(X_train, y_train):
      sample = x.flatten()
      y_hat, a, h = forward_prop(w, b, sample, n_layers, activation_f)
      temp_dw, temp_db = backprop(y_hat, y, a, h, w, b, activation_f, batch_size, loss, ud_lambda, n_outputs)
      num_points_seen = num_points_seen + 1
      for j in range(1, n_layers+2):
          dw[j] = dw[j]+ np.array(temp_dw[j])
          db[j] = db[j]+ np.array(temp_db[j])
      #print(num_points_seen)
      if (num_points_seen % batch_size) == 0:
        #print("update rule start: ", num_points_seen)
        for j in range(1, n_layers+2):
          mw[j] = (beta1*mw[j] + (1-beta1)*dw[j])/(1-np.power(beta1, i+1))
          mb[j] = (beta1*mb[j] + (1-beta1)*db[j])/(1-np.power(beta1, i+1))
          vw[j] = (beta2*vw[j] + (1-beta2)*dw[j]**2) /(1-np.power(beta2, i+1))
          vb[j] = (beta2*vb[j] + (1-beta2)*db[j]**2) /(1-np.power(beta2, i+1))

          w[j] = w[j]- (eta/np.sqrt(vw[j]+eps)) * (beta1*mw[j]+(1-beta1)*dw[j]/(1-beta1**(i+1)))
          b[j] = b[j]- (eta/np.sqrt(vb[j]+eps)) * (beta1*mb[j]+(1-beta1)*db[j]/(1-beta1**(i+1)))
        #print("update rule end: ", num_points_seen)
        for ii in range(1, n_layers+2):
          dw[ii] = np.zeros_like(w[ii])
          db[ii] = np.zeros_like(b[ii])

  return w,b,a,h

def mgd(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, max_epochs, momentum):
  print("in mgd")
  L = len(w)-1
  dw = [0]*(L+1)
  db = [0]*(L+1)

  prev_uw = [0]*(L+1)
  prev_ub = [0]*(L+1)
  uw = [0]*(L+1)
  ub = [0]*(L+1)

  for ii in range(1, n_layers+2):
    prev_uw[ii] = np.zeros_like(w[ii])
    prev_ub[ii] = np.zeros_like(b[ii])
    uw[ii] = np.zeros_like(w[ii])
    uw[ii] = np.zeros_like(b[ii])
  for i in range(max_epochs):
    print("epoch", i)
    for ii in range(1, n_layers+2):
      dw[ii] = np.zeros_like(w[ii])
      db[ii] = np.zeros_like(b[ii])
    num_points_seen = 0
    for x,y in zip(X_train, y_train):
      sample = x.flatten()
      y_hat, a, h = forward_prop(w, b, sample, n_layers, activation_f)
      temp_dw, temp_db = backprop(y_hat, y, a, h, w, b, activation_f, batch_size, loss, ud_lambda, n_outputs)
      num_points_seen = num_points_seen + 1
      for j in range(1, n_layers+2):
          dw[j] = dw[j]+ np.array(temp_dw[j])
          db[j] = db[j]+ np.array(temp_db[j])
      #print(num_points_seen)
      if (num_points_seen % batch_size) == 0:
        # print("update rule start: ", num_points_seen)
        for j in range(1, n_layers+2):
          uw[j] = momentum*prev_uw[j] + eta*dw[j]
          ub[j] = momentum*prev_ub[j] + eta*db[j]
          w[j] = (w[j]- uw[j])
          b[j] = b[j]- ub[j]
        for j in range(1, n_layers+2):
          prev_uw[j] = uw[j]
          prev_ub[j] = ub[j]
        # print("update rule end: ", num_points_seen)
        for ii in range(1, n_layers+2):
          dw[ii] = np.zeros_like(w[ii])
          db[ii] = np.zeros_like(b[ii])

  return w,b,a,h

def nag(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, use_wandb, max_epochs, momentum):
  print("in nag")
  L = len(w)-1
  prev_vw = [0]*(L+1)
  prev_vb = [0]*(L+1)
  wt = [0]*(L+1)
  bt = [0]*(L+1)
  vw = [0]*(L+1)
  vb = [0]*(L+1)
  dw = [0]*(L+1)
  db = [0]*(L+1)
  for ii in range(1, n_layers+2):
    prev_vw[ii] = np.zeros_like(w[ii])
    prev_vb[ii] = np.zeros_like(b[ii])

  for i in range(max_epochs):
    print("epoch", i)
    for ii in range(1, n_layers+2):
      dw[ii] = np.zeros_like(w[ii])
      db[ii] = np.zeros_like(b[ii])

    for jj in range(1, n_layers+2):
        vw[jj] = momentum*prev_vw[jj]
        vb[jj] = momentum*prev_vb[jj]
    num_points_seen = 0
    for x,y in zip(X_train, y_train):
      sample = x.flatten()
      for jk in range(1, n_layers+2):
        wt[jk] = w[jk] - vw[jk]
        bt[jk] = b[jk] - vb[jk]
      y_hat, a, h = forward_prop(wt, bt, sample, n_layers, activation_f)
      temp_dw, temp_db = backprop(y_hat, y, a, h, wt, bt, activation_f, batch_size, loss, ud_lambda, n_outputs)
      num_points_seen = num_points_seen + 1
      for jjk in range(1, n_layers+2):
          dw[jjk] = dw[jjk]+ np.array(temp_dw[jjk])
          db[jjk] = db[jjk]+ np.array(temp_db[jjk])
      #print(num_points_seen)
      if (num_points_seen % batch_size) == 0:
        # print("update rule start: ", num_points_seen)
        for j in range(1, n_layers+2):
          vw[j] = momentum*prev_vw[j] + dw[j]
          vb[j] = momentum*prev_vb[j] + db[j]
          w[j] = (w[j]- eta*vw[j])
          b[j] = b[j]- eta*vb[j]
        for j in range(1, n_layers+2):
          prev_vw[j] = vw[j]
          prev_vb[j] = vb[j]
        # print("update rule end: ", num_points_seen)
        for ii in range(1, n_layers+2):
          dw[ii] = np.zeros_like(w[ii])
          db[ii] = np.zeros_like(b[ii])

  return w,b,a,h

def rmsprop(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, use_wandb, max_epochs, beta, eps):
  print("in rmsprop")
  L = len(w)-1
  vw = [0]*(L+1)
  vb = [0]*(L+1)
  dw = [0]*(L+1)
  db = [0]*(L+1)
  for ii in range(1, n_layers+2):
    vw[ii] = np.zeros_like(w[ii])
    vb[ii] = np.zeros_like(b[ii])

  for i in range(max_epochs):
    print("epoch", i)
    for ii in range(1, n_layers+2):
      dw[ii] = np.zeros_like(w[ii])
      db[ii] = np.zeros_like(b[ii])

    num_points_seen = 0
    for x,y in zip(X_train, y_train):
      sample = x.flatten()
      y_hat, a, h = forward_prop(w, b, sample, n_layers, activation_f)
      temp_dw, temp_db = backprop(y_hat, y, a, h, w, b, activation_f, batch_size, loss, ud_lambda, n_outputs)

      num_points_seen = num_points_seen + 1
      for j in range(1, n_layers+2):
          dw[j] = dw[j]+ np.array(temp_dw[j])
          db[j] = db[j]+ np.array(temp_db[j])
      #print(num_points_seen)
      if (num_points_seen % batch_size) == 0:
        # print("update rule start: ", num_points_seen)
        for j in range(1, n_layers+2):
          vw[j] = beta*vw[j] + (1-beta)*dw[j]**2
          vb[j] = beta*vb[j] + (1-beta)*db[j]**2
          w[j] = (w[j]- eta*dw[j]/(np.sqrt(vw[j]+eps)))
          b[j] = (b[j]- eta*db[j]/(np.sqrt(vb[j]+eps)))
        # print("update rule end: ", num_points_seen)
        for ii in range(1, n_layers+2):
          dw[ii] = np.zeros_like(w[ii])
          db[ii] = np.zeros_like(b[ii])

  return w,b,a,h

def adam(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, use_wandb, max_epochs, beta1, beta2, eps):
  print("in adam")
  L = len(w)-1
  mw = [0]*(L+1)
  mb = [0]*(L+1)
  vw = [0]*(L+1)
  vb = [0]*(L+1)
  dw = [0]*(L+1)
  db = [0]*(L+1)
  for ii in range(1, n_layers+2):
    vw[ii] = np.zeros_like(w[ii])
    vb[ii] = np.zeros_like(b[ii])
    mw[ii] = np.zeros_like(w[ii])
    mb[ii] = np.zeros_like(b[ii])

  for i in range(max_epochs):
    print("epoch", i)
    for ii in range(1, n_layers+2):
      dw[ii] = np.zeros_like(w[ii])
      db[ii] = np.zeros_like(b[ii])
    num_points_seen = 0
    for x,y in zip(X_train, y_train):
      sample = x.flatten()
      y_hat, a, h = forward_prop(w, b, sample, n_layers, activation_f)
      temp_dw, temp_db = backprop(y_hat, y, a, h, w, b, activation_f, batch_size, loss, ud_lambda, n_outputs)

      num_points_seen = num_points_seen + 1
      for jk in range(1, n_layers+2):
        dw[jk] = dw[jk]+ np.array(temp_dw[jk])
        db[jk] = db[jk]+ np.array(temp_db[jk])
      #print(num_points_seen)
      if (num_points_seen % batch_size) == 0:
        # print("update rule start: ", num_points_seen)
        for j in range(1, n_layers+2):
          mw[j] = (beta1*mw[j] + (1-beta1)*dw[j])/(1-np.power(beta1, i+1) )
          mb[j] = (beta1*mb[j] + (1-beta1)*db[j])/(1-np.power(beta1, i+1) )
          vw[j] = (beta2*vw[j] + (1-beta2)*dw[j]**2) /(1-np.power(beta2, i+1))
          vb[j] = (beta2*vb[j] + (1-beta2)*db[j]**2) /(1-np.power(beta2, i+1))
          w[j] = w[j]- eta*mw[j]/(np.sqrt(vw[j]+eps))
          b[j] = b[j]- eta*mb[j]/(np.sqrt(vb[j]+eps))
        # print("update rule end: ", num_points_seen)
        for ii in range(1, n_layers+2):
          dw[ii] = np.zeros_like(w[ii])
          db[ii] = np.zeros_like(b[ii])

  return w,b,a,h

# seed(42)
# n_inputs = 784
# n_layers = 4
# n_neurons = 128
# n_outputs = 10
# init_mode = "xavier"
# activation_f = "tanh"
# w,b = init_nn(n_inputs, n_layers, n_neurons, n_outputs, init_mode)
# w_old = w.copy()
# b_old = b.copy()

# eta = 0.001

# batch_size = 32
# loss = "crossentropy"
# ud_lambda = 0.001
# beta = 0.9
# beta1 = 0.9
# beta2 = 0.999
# eps = 1e-10
# max_epochs = 3
# momentum = 0.9

# #print("before sgd  call")
# #w_n,b_n, a_n, h_n = sgd(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, use_wandb=True)
# #w_n,b_n, a_n, h_n = mgd(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, max_epochs, momentum)
# #w_n,b_n, a_n, h_n = nag(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, momentum)
# #w_n,b_n, a_n, h_n = rmsprop(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, beta, eps)
# #w_n,b_n, a_n, h_n = adam(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, beta1, beta2, eps)
# #w_n,b_n, a_n, h_n = nadam(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, beta1, beta2, eps)
# w_n,b_n, a_n, h_n = mgd(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, max_epochs, momentum)
# train_accuracy, yp = accuracy(X_train, y_train, w_n, b_n, n_layers, activation_f)
# validation_accuracy, yp = accuracy(X_validation, y_validation, w_n, b_n, n_layers, activation_f)
# test_accuracy, yp = accuracy(X_test, y_test, w_n, b_n, n_layers, activation_f)
# print(train_accuracy)
# print(validation_accuracy)
# print(test_accuracy)

# aa = np.array([[1,2],[3,4]])
# aa**2

# import numpy as np
  # w = [[],[[2,3],[2,3], [4,5]],[[2,3],[2,3]]]
  # b = [[],[1,1],[1,1]]
  # L = len(w)-1
  # vw = [0]*(L+1)
  # vb = [0]*(L+1)
  # for ii in range(1, 1+2):
  #   vw[ii] = np.zeros_like(w[ii])
  #   vb[ii] = np.zeros_like(b[ii])
  #   print(vw[ii].shape)

sweep_config = {"name": "complete-sweep-25dw", "method": "random"}
sweep_config["metric"] = {"name": "validation_acc", "goal": "maximize"}
parameters_dict = {
                "num_epochs": {"values": [2, 3, 4, 5]}, \
                "num_hidden_layers": {"values": [2, 3, 4]}, \
                "size_hidden_layer": {"values": [16, 32, 64, 128]}, \
                "learning_rate": {"values": [ 1e-2, 1e-3, 1e-4]}, \
                "optimizer": {"values": ["sgd", "adam", "mgd", "nadam", "rmsprop", "nag"]}, \
                "batch_size": {"values": [32, 64, 128]}, \
                "weight_init": {"values": ["random", "xavier"]} , \
                "activation": {"values": ["sigmoid", "tanh", "relu"]}, \
                "loss": {"values": ["crossentropy"]}, \
                "reg_lambda": {"values": [0.001, 0.0005, 0]}, \
                  }
sweep_config["parameters"] = parameters_dict

def train_nn():
    config_defaults = {
            "num_epochs": 3, \
            "num_hidden_layers":  4, \
            "size_hidden_layer": 128, \
            "learning_rate": 1e-3, \
            "optimizer": "mgd", \
            "batch_size": 32, \
            "weight_init": "xavier" , \
            "activation": "tanh", \
            "loss": "crossentropy", \
            "reg_lambda": 0.001, \
    }
    wandb.init(config = config_defaults, project = "Assignment1")
    config = wandb.config
    print(config.optimizer)
    wandb.run.name = "e_{}_hl_{}_opt_{}_bs_{}_init_{}_ac_{}_loss_{}".format(config.num_epochs,\
                                                                  config.num_hidden_layers,\
                                                                  config.size_hidden_layer,\
                                                                  config.learning_rate,\
                                                                  config.optimizer,\
                                                                  config.batch_size,\
                                                                  config.weight_init,\
                                                                  config.activation,\
                                                                  config.loss,\
                                                                  config.reg_lambda)

    seed(42)
    n_inputs = 784
    n_layers = config.num_hidden_layers
    n_neurons = config.size_hidden_layer
    n_outputs = 10
    init_mode = config.weight_init
    w,b = init_nn(n_inputs, n_layers, n_neurons, n_outputs, init_mode)
    w_old = w.copy()
    b_old = b.copy()

    eta = config.learning_rate
    activation_f = config.activation
    batch_size = config.batch_size
    loss = config.loss
    ud_lambda = config.reg_lambda
    optimizer = config.optimizer

    beta = 0.9
    beta1 = 0.9
    beta2 = 0.999

    eps = 1e-9
    max_epochs = config.num_epochs
    momentum = 0.9



    wandb.log({"examples": fig1})



    if optimizer == "sgd":
      w_n,b_n, a_n, h_n = sgd(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs)
    elif optimizer == "mgd":
      w_n,b_n, a_n, h_n = mgd(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, max_epochs, momentum)
    elif optimizer == "nag":
      w_n,b_n, a_n, h_n = nag(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, momentum)
    elif optimizer == "rmsprop":
      w_n,b_n, a_n, h_n = rmsprop(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, beta, eps)
    elif optimizer == "adam":
      w_n,b_n, a_n, h_n = adam(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, beta1, beta2, eps)
    elif optimizer == "nadam":
      w_n,b_n, a_n, h_n = nadam(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, beta1, beta2, eps)


    train_accuracy, yp = accuracy(X_train, y_train, w_n, b_n, n_layers, activation_f)
    validation_accuracy, yp = accuracy(X_validation, y_validation, w_n, b_n, n_layers, activation_f)
    test_accuracy, ytp = accuracy(X_test, y_test, w_n, b_n, n_layers, activation_f)

    cm = wandb.plot.confusion_matrix(y_true=y_test, preds=ytp, class_names=u_l)
    wandb.log({"conf_mat": cm})


    wandb.log({"train_acc": train_accuracy, \
               "validation_acc": validation_accuracy, \
                "test_acc": test_accuracy, \
                })
    print("accu: ", validation_accuracy)




####################################################################

#sweep_id = wandb.sweep(sweep_config, project = "Assignment1")
#wandb.agent(sweep_id, function = train_nn, count=25)
# wandb.finish()
#sweep_id = wandb.sweep(project = "Assignment1")
#wandb.agent(function = train_nn)
#wandb.finish()
#train_nn()
#wandb.finish()

np.array(samples_for_print).shape
from PIL import Image
images = [Image.fromarray(image) for image in samples_for_print]

np.array(samples_for_print).shape
y = np.expand_dims(samples_for_print, axis=3)
y.shape

# def train_nn():
#     config_defaults = {
#             "num_epochs": 3, \
#             "num_hidden_layers":  4, \
#             "size_hidden_layer": 128, \
#             "learning_rate": 1e-3, \
#             "optimizer": "mgd", \
#             "batch_size": 32, \
#             "weight_init": "xavier" , \
#             "activation": "tanh", \
#             "loss": "crossentropy", \
#             "reg_lambda": 0.001, \
#     }
#     wandb.init(config = config_defaults, project = "Assignment1")
#     config = wandb.config
#     print(config.optimizer)
#     wandb.run.name = "e_{}_hl_{}_opt_{}_bs_{}_init_{}_ac_{}_loss_{}".format(config.num_epochs,\
#                                                                   config.num_hidden_layers,\
#                                                                   config.size_hidden_layer,\
#                                                                   config.learning_rate,\
#                                                                   config.optimizer,\
#                                                                   config.batch_size,\
#                                                                   config.weight_init,\
#                                                                   config.activation,\
#                                                                   config.loss,\
#                                                                   config.reg_lambda)

#     seed(42)
#     n_inputs = 784
#     n_layers = config.num_hidden_layers
#     n_neurons = config.size_hidden_layer
#     n_outputs = 10
#     init_mode = config.weight_init
#     w,b = init_nn(n_inputs, n_layers, n_neurons, n_outputs, init_mode)
#     w_old = w.copy()
#     b_old = b.copy()

#     eta = config.learning_rate
#     activation_f = config.activation
#     batch_size = config.batch_size
#     loss = config.loss
#     ud_lambda = config.reg_lambda
#     optimizer = config.optimizer

#     beta = 0.9
#     beta1 = 0.9
#     beta2 = 0.999

#     eps = 1e-9
#     max_epochs = config.num_epochs
#     momentum = 0.9

#     if optimizer == "sgd":
#       w_n,b_n, a_n, h_n = sgd(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs)
#     elif optimizer == "mgd":
#       w_n,b_n, a_n, h_n = mgd(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, max_epochs, momentum)
#     elif optimizer == "nag":
#       w_n,b_n, a_n, h_n = nag(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, momentum)
#     elif optimizer == "rmsprop":
#       w_n,b_n, a_n, h_n = rmsprop(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, beta, eps)
#     elif optimizer == "adam":
#       w_n,b_n, a_n, h_n = adam(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, beta1, beta2, eps)
#     elif optimizer == "nadam":
#       w_n,b_n, a_n, h_n = nadam(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, beta1, beta2, eps)


#     train_accuracy, yp = accuracy(X_train, y_train, w_n, b_n, n_layers, activation_f)
#     validation_accuracy, yp = accuracy(X_validation, y_validation, w_n, b_n, n_layers, activation_f)
#     test_accuracy, ytp = accuracy(X_test, y_test, w_n, b_n, n_layers, activation_f)


#     wandb.log({"train_acc": train_accuracy, \
#                "validation_acc": validation_accuracy, \
#                 "test_acc": test_accuracy, \
#                 })
#     print("accu: ", validation_accuracy)


# ####################################################################

# #sweep_id = wandb.sweep(sweep_config, project = "Assignment1")
# #wandb.agent(sweep_id, function = train_nn, count=25)
# # wandb.finish()
# #sweep_id = wandb.sweep(project = "Assignment1")
# #wandb.agent(function = train_nn)
# #wandb.finish()
# train_nn()
# wandb.finish()





# print(train_accuracy*100)
# print(test_accuracy*100)

# plt.plot(w_old[n_layers][1], 'o')
# plt.plot(w_n[n_layers][1], 'o')
# # plt.show()

# plt.plot(b_old[1], 'o')
# plt.plot(b_n[1], 'o')
# plt.show()

# sample = X_train[0].flatten()
# y_hat, a, h = forward_prop(w, b, sample, n_layers)
# dw, db = backprop(y_hat, y_train[0], a, h, w, b, activation_f, batch_size, loss, ud_lambda, n_outputs)

# for i in range(0,n_layers+2):
#   print(np.array(b[i]).shape)